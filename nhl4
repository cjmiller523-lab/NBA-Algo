# nhl4.py
# ------------------------------------------------------------
# NHL Hit-Rate Engine (schedule -> logs cache -> hit rates -> odds merge)
# Outputs:
#   - Game-by-game top 3 props per stat
#   - Global 100% locks (within ODDS_WINDOW)
#   - Bounce-back candidates
#   - Twitter-ready text file in twitter_posts/
# ------------------------------------------------------------
import os
import sys
import json
import time
import math
import argparse
from datetime import datetime, timedelta
from collections import defaultdict

import requests

# =============== SETTINGS ===============
LAST_N_GAMES = 10
SEASON = "20242025"   # NHL season format, e.g., 20242025
MAX_RETRIES = 3
RETRY_SLEEP = 1.2

CACHE_DIR = "cache_nhl"
LOGS_DIR = os.path.join(CACHE_DIR, "logs", SEASON)
ROSTERS_PATH = "nhl_rosters.json"          # optional; auto-builds if missing
INJURIES_PATH = "nhl_out_statuses.json"    # optional; list[str] of full player names to exclude
ODDS_CACHE_DIR = "odds_cache"              # expects nhl_YYYY-MM-DD.json inside
TWITTER_DIR = "twitter_posts"

# Markets to compute hit-rates for:
MARKETS = ["SOG", "PTS", "AST", "GLS"]  # shots on goal, points, assists, goals

# Threshold ladders per market (display thresholds)
THRESHOLDS = {
    "SOG": [1, 2, 3, 4, 5, 6, 7],
    "PTS": [0.5, 1.5, 2.5, 3.5],
    "AST": [0.5, 1.5, 2.5],
    "GLS": [0.5, 1.5, 2.5]
}

# Global locks filtering
ODDS_WINDOW = (-1500, -150)   # inclusive low/high (American odds)
REQUIRE_100 = True            # global list only prints 10/10 (or LAST_N_GAMES/LAST_N_GAMES)

# Max items per game/market for the display section
TOP_K_PER_GAME = 3

# =============== HELPERS ===============

def ensure_dirs():
    os.makedirs(LOGS_DIR, exist_ok=True)
    os.makedirs(TWITTER_DIR, exist_ok=True)
    os.makedirs(ODDS_CACHE_DIR, exist_ok=True)

def http_get(url, params=None, retries=MAX_RETRIES, sleep=RETRY_SLEEP):
    last = None
    for i in range(retries):
        try:
            r = requests.get(url, params=params, timeout=20)
            if r.status_code == 200:
                return r.json()
            last = f"HTTP {r.status_code} for {url}"
        except requests.RequestException as e:
            last = str(e)
        time.sleep(sleep * (i + 1))
    raise RuntimeError(last or f"GET failed for {url}")

def nhl_schedule(date_str):
    url = "https://statsapi.web.nhl.com/api/v1/schedule"
    data = http_get(url, params={"date": date_str})
    games = []
    for d in data.get("dates", []):
        for g in d.get("games", []):
            away = g["teams"]["away"]["team"]["abbreviation"]
            home = g["teams"]["home"]["team"]["abbreviation"]
            games.append({
                "gamePk": g["gamePk"],
                "away": away,
                "home": home
            })
    return games

def nhl_team_roster(team_id):
    # Returns [{id, fullName, position}]
    url = f"https://statsapi.web.nhl.com/api/v1/teams/{team_id}?expand=team.roster"
    data = http_get(url)
    teams = data.get("teams", [])
    if not teams:
        return []
    roster = teams[0].get("roster", {}).get("roster", [])
    out = []
    for p in roster:
        person = p.get("person", {})
        pos = p.get("position", {})
        out.append({
            "id": person.get("id"),
            "fullName": person.get("fullName"),
            "position": pos.get("abbreviation")
        })
    return out

def nhl_all_teams():
    data = http_get("https://statsapi.web.nhl.com/api/v1/teams")
    teams = []
    for t in data.get("teams", []):
        teams.append({
            "id": t["id"],
            "name": t["name"],
            "abbrev": t["abbreviation"]
        })
    return teams

def build_or_load_rosters(path=ROSTERS_PATH):
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)

    print("üîß Building nhl_rosters.json (first run)‚Ä¶")
    teams = nhl_all_teams()
    mapping = {}
    for t in teams:
        try:
            roster = nhl_team_roster(t["id"])
            for p in roster:
                mapping[p["fullName"]] = {
                    "id": p["id"],
                    "team": t["abbrev"],
                    "pos": p.get("position")
                }
            time.sleep(0.15)
        except Exception as e:
            print(f"  ‚ö†Ô∏è Roster fetch failed for {t['abbrev']}: {e}")

    with open(path, "w", encoding="utf-8") as f:
        json.dump(mapping, f, indent=2, ensure_ascii=False)
    return mapping

def load_injuries(path=INJURIES_PATH):
    if os.path.exists(path):
        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
            if isinstance(data, list):
                return set(data)
        except:
            pass
    return set()

def player_gamelog_cached(player_id, season=SEASON):
    fp = os.path.join(LOGS_DIR, f"{player_id}.json")
    if os.path.exists(fp):
        with open(fp, "r", encoding="utf-8") as f:
            return json.load(f)
    return None

def save_player_gamelog(player_id, payload, season=SEASON):
    fp = os.path.join(LOGS_DIR, f"{player_id}.json")
    with open(fp, "w", encoding="utf-8") as f:
        json.dump(payload, f, indent=2, ensure_ascii=False)

def fetch_player_gamelog(player_id, season=SEASON):
    # NHL stats: https://statsapi.web.nhl.com/api/v1/people/{id}/stats?stats=gameLog&season=YYYYYYYY
    url = f"https://statsapi.web.nhl.com/api/v1/people/{player_id}/stats"
    data = http_get(url, params={"stats": "gameLog", "season": season})
    return data

def extract_recent_stats(gamelog_json, last_n=LAST_N_GAMES):
    """
    Returns a list of dicts with fields: date, team, opp, sog, g, a, pts
    Most recent first as provided by API; we‚Äôll slice to last_n.
    """
    records = []
    splits = (gamelog_json.get("stats") or [{}])[0].get("splits", [])
    for s in splits:
        stat = s.get("stat", {})
        team = s.get("team", {}).get("abbreviation")
        opp  = s.get("opponent", {}).get("abbreviation")
        date = s.get("date")
        sog = stat.get("shots", 0)  # "shots" in NHL API is SOG
        g = stat.get("goals", 0)
        a = stat.get("assists", 0)
        pts = stat.get("points", 0)
        records.append({
            "date": date, "team": team, "opp": opp,
            "SOG": sog, "GLS": g, "AST": a, "PTS": pts
        })
    # API returns most recent first; slice to last_n
    return records[:last_n]

def to_american_odds(decimal):
    # Helper if someone wants to convert, not used unless needed
    if decimal <= 0:
        return None
    if decimal >= 2.0:
        return int((decimal - 1) * 100)
    return int(-100 / (decimal - 1))

def load_odds_cache_for_date(date_str):
    # Expected path: odds_cache/nhl_YYYY-MM-DD.json
    # Minimal expected schema (example):
    # {
    #   "2025-12-07": {
    #       "Brock Boeser": {
    #           "SOG": [{"line": 3.5, "odds": -120}],
    #           "PTS": [{"line": 0.5, "odds": -140}],
    #           "AST": [{"line": 0.5, "odds": +125}],
    #           "GLS": [{"line": 0.5, "odds": +110}]
    #       },
    #       ...
    #   }
    # }
    fp = os.path.join(ODDS_CACHE_DIR, f"nhl_{date_str}.json")
    if os.path.exists(fp):
        try:
            with open(fp, "r", encoding="utf-8") as f:
                root = json.load(f)
            return root.get(date_str, {})
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to parse odds cache: {e}")
    return {}

def american_in_window(odds, low=ODDS_WINDOW[0], high=ODDS_WINDOW[1]):
    try:
        return (odds is not None) and (low <= odds <= high)
    except:
        return False

def fmt_american(odds):
    if odds is None:
        return "N/A"
    if odds > 0:
        return f"+{odds}"
    return str(odds)

def pick_best_price(entries):
    # entries: list of {line, odds}. Choose the one closest to our thresholds logic
    # For now just pick the first (or best book price by absolute value).
    if not entries:
        return None
    # Prefer the most favorable odds (highest for +, lowest absolute for -)
    best = entries[0]
    for e in entries[1:]:
        # pick the higher implied probability (more negative), but you can change
        if e.get("odds", 0) < best.get("odds", 0):
            best = e
    return best

# =============== CORE PIPELINE ===============

def compute_hit_rate(recent_list, market, threshold):
    # Count games where stat >= threshold
    stat_values = [r[market] for r in recent_list]
    hits = sum(1 for v in stat_values if v >= threshold)
    total = len(stat_values)
    return hits, total

def best_threshold_hit(recent_list, market):
    # Find the highest threshold with the best (hits/total, ties break by threshold)
    best = None
    for th in THRESHOLDS[market]:
        hits, total = compute_hit_rate(recent_list, market, th)
        ratio = hits / total if total else 0.0
        candidate = (ratio, th, hits, total)
        if (best is None) or (candidate > best):
            best = candidate
    return best  # (ratio, threshold, hits, total)

def bounce_back_flag(recent_list, market, high_bar=0.8):
    # Bounce-back: last game miss, but overall >= high_bar
    if not recent_list:
        return False
    last = recent_list[0][market]  # most recent game is index 0
    # Decide a "primary" threshold for that market (basic)
    default_th = THRESHOLDS[market][min(2, len(THRESHOLDS[market]) - 1)]  # e.g., SOG: 3
    last_hit = (last >= default_th)
    hits, total = compute_hit_rate(recent_list, market, default_th)
    overall = hits / total if total else 0.0
    return (not last_hit) and (overall >= high_bar)

def main():
    ensure_dirs()

    parser = argparse.ArgumentParser(description="NHL Hit-Rate Engine")
    parser.add_argument("--date", type=str, default=datetime.now().strftime("%Y-%m-%d"),
                        help="Run for a specific date (YYYY-MM-DD)")
    parser.add_argument("--season", type=str, default=SEASON,
                        help="Season code YYYYYYYY, e.g. 20242025")
    parser.add_argument("--last", type=int, default=LAST_N_GAMES, help="Games window for hit rate")
    parser.add_argument("--save-twitter", action="store_true", help="Force save twitter text")
    args = parser.parse_args()

    date_str = args.date
    season = args.season
    last_n = max(1, args.last)

    # Load resources
    rosters = build_or_load_rosters(ROSTERS_PATH)
    out_set = load_injuries(INJURIES_PATH)
    odds_map = load_odds_cache_for_date(date_str)

    # Schedule
    print("üîç Fetching NHL schedule‚Ä¶")
    games = nhl_schedule(date_str)
    print(f"Number of games: {len(games)}")
    if not games:
        txt_path = os.path.join(TWITTER_DIR, f"{date_str}.txt")
        with open(txt_path, "w", encoding="utf-8") as f:
            f.write(f"{date_str}\nNo NHL games.")
        print(f"üìÑ Twitter text saved to {txt_path}")
        return

    # Build quick team->players from roster map
    team_to_players = defaultdict(list)
    for name, meta in rosters.items():
        team_to_players[meta["team"]].append((name, meta["id"]))

    # Aggregate results
    per_game_results = defaultdict(lambda: defaultdict(list))  # game_key -> market -> list of rows
    global_100 = []  # (name, team, market, th, hits, total, odds, game_key)
    bounce_candidates = []  # (name, team, market, game_key)

    # For each team in today‚Äôs games, iterate roster players and compute per-market hit rates
    all_teams_today = set()
    for g in games:
        all_teams_today.add(g["home"])
        all_teams_today.add(g["away"])

    print("üì• Pulling player logs (cache-first)‚Ä¶")
    for team in sorted(all_teams_today):
        players = team_to_players.get(team, [])
        if not players:
            continue
        for (full_name, pid) in players:
            if full_name in out_set:
                continue

            # Fetch/cached logs
            payload = player_gamelog_cached(pid, season=season)
            if payload is None:
                try:
                    payload = fetch_player_gamelog(pid, season=season)
                    save_player_gamelog(pid, payload, season=season)
                    time.sleep(0.06)
                except Exception as e:
                    # skip quietly if no logs
                    continue

            recent = extract_recent_stats(payload, last_n=last_n)
            if not recent:
                continue

            # Determine today‚Äôs opponent and game key if applicable
            # We use the player's team and check matchups in games
            # Make a simple map for today
            game_key = None
            for gm in games:
                if team == gm["home"]:
                    game_key = f"{gm['away']}@{gm['home']}"
                    opp = gm["away"]
                    break
                if team == gm["away"]:
                    game_key = f"{gm['away']}@{gm['home']}"
                    opp = gm["home"]
                    break
            if not game_key:
                continue

            # Merge odds (if any)
            player_odds = odds_map.get(full_name, {})

            # Markets loop
            for M in MARKETS:
                # best threshold summary
                ratio, th, hits, total = best_threshold_hit(recent, M)
                # select an odds entry for that market if available
                best_odds_entry = pick_best_price(player_odds.get(M, [])) if player_odds else None
                american = best_odds_entry.get("odds") if best_odds_entry else None

                row = {
                    "player": full_name,
                    "team": team,
                    "market": M,
                    "threshold": th,
                    "hits": hits,
                    "total": total,
                    "hitrate": round(100 * (hits / total if total else 0.0), 1),
                    "odds": american,
                }
                per_game_results[game_key][M].append(row)

                # Global 100% locks (10/10) within odds window
                if REQUIRE_100 and total == last_n and hits == last_n and american_in_window(american):
                    global_100.append((full_name, team, M, th, hits, total, american, game_key))

                # Bounce-back
                if bounce_back_flag(recent, M, high_bar=0.8):
                    bounce_candidates.append((full_name, team, M, game_key))

    # Sort per-game per-market and trim
    for gk in per_game_results:
        for M in per_game_results[gk]:
            per_game_results[gk][M].sort(key=lambda r: (r["hits"] / (r["total"] or 1.0), r["threshold"]), reverse=True)
            per_game_results[gk][M] = per_game_results[gk][M][:TOP_K_PER_GAME]

    # Sort global locks by odds tightness then name
    global_100.sort(key=lambda t: (abs(t[6]) if t[6] is not None else 10_000, t[0]))

    # =============== PRINT OUTPUT ===============
    print("\n==============================")
    print(f"üèí NHL Props ‚Äî {date_str}")
    print("==============================\n")

    for g in games:
        gk = f"{g['away']}@{g['home']}"
        print(f"==============================\n{gk}\n==============================")
        gm = per_game_results.get(gk, {})
        for M in MARKETS:
            arr = gm.get(M, [])
            if not arr:
                continue
            header = {
                "SOG": "Shots on Goal",
                "PTS": "Points",
                "AST": "Assists",
                "GLS": "Goals"
            }[M]
            print(f"\n{header}")
            for r in arr:
                odds_str = fmt_american(r["odds"])
                print(f"{r['player']} ({r['team']}) - {header} {r['threshold']}+ @ {odds_str} | {r['hits']}/{r['total']} ({r['hitrate']}%)")
        print()

    # Global 100% list
    print("\nüî• 100% Locks Today ‚Äì Global List")
    if not global_100:
        print("No players hit 100% within the odds window.")
    else:
        for (name, team, M, th, hits, total, american, gk) in global_100:
            label = {"SOG":"SOG","PTS":"pts","AST":"ast","GLS":"gls"}[M]
            print(f"{name} {label} {th}+ @ {fmt_american(american)} | {hits}/{total} ‚Äî {gk}")

    # Bounce-back
    print("\nüîÅ Global Bounce-Back Candidates (missed last, ‚â•80% overall)")
    if not bounce_candidates:
        print("No bounce-back candidates today.")
    else:
        # de-dup by player+market
        seen = set()
        for (name, team, M, gk) in bounce_candidates:
            key = (name, M)
            if key in seen:
                continue
            seen.add(key)
            tag = {"SOG":"SOG","PTS":"PTS","AST":"AST","GLS":"GLS"}[M]
            print(f"{name} ({team}) ‚Äî {tag} ‚Äî {gk}")

    # =============== TWITTER TEXT SAVE ===============
    lines = []
    lines.append(f"{date_str} ‚Äî NHL")
    for g in games:
        gk = f"{g['away']}@{g['home']}"
        gm = per_game_results.get(gk, {})
        any_market = any(gm.get(M) for M in MARKETS)
        if not any_market:
            continue
        lines.append("")
        lines.append(f"{gk}")
        for M in MARKETS:
            arr = gm.get(M, [])
            if not arr:
                continue
            header = {
                "SOG": "Shots on Goal",
                "PTS": "Points",
                "AST": "Assists",
                "GLS": "Goals"
            }[M]
            lines.append(header)
            for r in arr:
                odds_str = fmt_american(r["odds"])
                # Keep concise for tweet friendliness
                lines.append(f"{r['player']} {header.split()[0]} {r['threshold']}+ @ {odds_str} | {r['hits']}/{r['total']}")

    lines.append("\n100% Locks (odds in window)")
    if not global_100:
        lines.append("None today.")
    else:
        for (name, team, M, th, hits, total, american, gk) in global_100[:12]:
            short = {"SOG":"SOG","PTS":"PTS","AST":"AST","GLS":"GLS"}[M]
            lines.append(f"{name} {short} {th}+ {fmt_american(american)} ‚Äî {gk}")

    txt_path = os.path.join(TWITTER_DIR, f"{date_str}.txt")
    with open(txt_path, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))
    print(f"\nüìÑ Twitter text saved to {txt_path}\n")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nInterrupted.")
